{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg4jukjO10-1"
      },
      "source": [
        "# Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzhbgrHzzTkT",
        "outputId": "6a851621-f70b-4b98-c38d-439e573dec2a"
      },
      "outputs": [],
      "source": [
        "# loading libraries\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# file paths (Colab environment)\n",
        "DATA_PATH = \"/content/PDA_Data.xlsx\"\n",
        "OUT_PATH = \"/content/PDA_Data_imputed_median.xlsx\"\n",
        "\n",
        "# main target and features used for grouping\n",
        "TARGET_COL = \"favourite_dish_price\"\n",
        "FEATURE_COLS = [\"city\", \"Services\", \"Restaurantkonzept\", \"Küchenregion\", \"Kettenzuordnung\"]\n",
        "\n",
        "# reproducibility + CV setup\n",
        "RANDOM_SEED = 42\n",
        "N_SPLITS = 5\n",
        "\n",
        "# thresholds for grouping stability\n",
        "MIN_CITY_COUNT = 20          # minimum number of observations per city before keeping its own label\n",
        "MIN_TRIPLE_COUNT = 10        # minimum size for (city, region, concept) group\n",
        "CITY_SMOOTH_K = 20.0         # smoothing factor for city-level fallback\n",
        "\n",
        "\n",
        "def normalize_text(x):\n",
        "    # basic cleanup for categorical fields (remove extra spaces etc.)\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    s = str(x).strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def to_numeric_price(series: pd.Series) -> pd.Series:\n",
        "    # convert price strings like \"12,50 €\" or \"12.50 EUR\" into float\n",
        "    def _parse(v):\n",
        "        if pd.isna(v):\n",
        "            return np.nan\n",
        "        t = str(v).strip()\n",
        "        if t == \"\":\n",
        "            return np.nan\n",
        "\n",
        "        # remove currency symbols and unify decimal separator\n",
        "        t = t.replace(\"€\", \"\").replace(\"EUR\", \"\").strip()\n",
        "        t = t.replace(\",\", \".\")\n",
        "        t = re.sub(r\"[^0-9.\\-]\", \"\", t)\n",
        "\n",
        "        try:\n",
        "            return float(t)\n",
        "        except Exception:\n",
        "            # fallback in case parsing fails\n",
        "            return np.nan\n",
        "\n",
        "    return series.map(_parse).astype(float)\n",
        "\n",
        "\n",
        "def reduce_rare_levels(series: pd.Series, min_count: int, other_label: str = \"Other\") -> pd.Series:\n",
        "    # group very small categories into \"Other\" to avoid unstable medians\n",
        "    vc = series.value_counts(dropna=True)\n",
        "    keep = set(vc[vc >= min_count].index.astype(str).tolist())\n",
        "\n",
        "    def _map(v):\n",
        "        if pd.isna(v):\n",
        "            return np.nan\n",
        "        sv = str(v)\n",
        "        return sv if sv in keep else other_label\n",
        "\n",
        "    return series.map(_map)\n",
        "\n",
        "\n",
        "def make_strat_bins(y: np.ndarray, n_bins: int = 10) -> np.ndarray:\n",
        "    # create quantile-based bins for stratified CV on continuous target\n",
        "    y = np.asarray(y, dtype=float)\n",
        "    ok = y[~np.isnan(y)]\n",
        "    if ok.size == 0:\n",
        "        return np.zeros_like(y, dtype=int)\n",
        "\n",
        "    q = np.linspace(0, 1, n_bins + 1)\n",
        "    edges = np.quantile(ok, q)\n",
        "    edges = np.unique(edges)\n",
        "\n",
        "    if len(edges) <= 2:\n",
        "        # fallback if not enough variation\n",
        "        return np.zeros_like(y, dtype=int)\n",
        "\n",
        "    return np.digitize(y, edges[1:-1], right=True)\n",
        "\n",
        "\n",
        "def eval_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
        "    # simple regression metrics for evaluation\n",
        "    mae = float(mean_absolute_error(y_true, y_pred))\n",
        "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "    return {\"MAE\": mae, \"RMSE\": rmse}\n",
        "\n",
        "\n",
        "def fit_imputer(train_df: pd.DataFrame, target_col: str):\n",
        "    # compute global median as ultimate fallback\n",
        "    global_med = float(np.nanmedian(train_df[target_col].values))\n",
        "\n",
        "    grp2_cols = [\"Küchenregion\", \"Restaurantkonzept\"]\n",
        "    grp3_cols = [\"city\", \"Küchenregion\", \"Restaurantkonzept\"]\n",
        "\n",
        "    # hierarchical group statistics\n",
        "    g2 = train_df.groupby(grp2_cols, dropna=False)[target_col].agg([\"median\", \"count\"])\n",
        "    g3 = train_df.groupby(grp3_cols, dropna=False)[target_col].agg([\"median\", \"count\"])\n",
        "    city_stats = train_df.groupby(\"city\", dropna=False)[target_col].agg([\"median\", \"count\"])\n",
        "\n",
        "    def city_smoothed(city):\n",
        "        # smoothed city-level estimate (Bayesian-style shrinkage to global median)\n",
        "        if city in city_stats.index:\n",
        "            m = float(city_stats.loc[city, \"median\"])\n",
        "            c = float(city_stats.loc[city, \"count\"])\n",
        "            if np.isnan(m):\n",
        "                return global_med\n",
        "            return (m * c + global_med * CITY_SMOOTH_K) / (c + CITY_SMOOTH_K)\n",
        "        return global_med\n",
        "\n",
        "    def predict_row(row):\n",
        "        # first try full triple (city, region, concept)\n",
        "        k3 = (row[\"city\"], row[\"Küchenregion\"], row[\"Restaurantkonzept\"])\n",
        "        if k3 in g3.index:\n",
        "            med3 = g3.loc[k3, \"median\"]\n",
        "            cnt3 = g3.loc[k3, \"count\"]\n",
        "            if pd.notna(med3) and float(cnt3) >= MIN_TRIPLE_COUNT:\n",
        "                return float(med3)\n",
        "\n",
        "        # then fallback to (region, concept)\n",
        "        k2 = (row[\"Küchenregion\"], row[\"Restaurantkonzept\"])\n",
        "        if k2 in g2.index:\n",
        "            med2 = g2.loc[k2, \"median\"]\n",
        "            if pd.notna(med2):\n",
        "                return float(med2)\n",
        "\n",
        "        # final fallback: smoothed city estimate\n",
        "        return float(city_smoothed(row[\"city\"]))\n",
        "\n",
        "    return global_med, predict_row\n",
        "\n",
        "\n",
        "def impute_prices(train_df: pd.DataFrame, df_to_impute: pd.DataFrame, target_col: str) -> np.ndarray:\n",
        "    # fit hierarchical medians on train and apply to new data\n",
        "    _, pred_fn = fit_imputer(train_df, target_col)\n",
        "    return df_to_impute.apply(pred_fn, axis=1).astype(float).values\n",
        "\n",
        "\n",
        "def main():\n",
        "    # read Excel file\n",
        "    df = pd.read_excel(DATA_PATH)\n",
        "\n",
        "    # normalize categorical text columns\n",
        "    for c in FEATURE_COLS:\n",
        "        df[c] = df[c].map(normalize_text)\n",
        "\n",
        "    # parse price column\n",
        "    df[TARGET_COL] = to_numeric_price(df[TARGET_COL])\n",
        "\n",
        "    # reduce rare cities\n",
        "    df[\"city\"] = reduce_rare_levels(df[\"city\"], min_count=MIN_CITY_COUNT, other_label=\"Other\")\n",
        "\n",
        "    # fill remaining missing categorical values\n",
        "    for c in FEATURE_COLS:\n",
        "        df[c] = df[c].fillna(\"Missing\").astype(str)\n",
        "\n",
        "    # split into labeled and unlabeled parts\n",
        "    missing_mask = df[TARGET_COL].isna()\n",
        "    labeled = df[~missing_mask].copy()\n",
        "    unlabeled = df[missing_mask].copy()\n",
        "\n",
        "    # prepare stratified CV on continuous target\n",
        "    y = labeled[TARGET_COL].values.astype(float)\n",
        "    bins = make_strat_bins(y, n_bins=10)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
        "    oof = np.zeros_like(y, dtype=float)\n",
        "\n",
        "    # cross-validation loop\n",
        "    for tr_idx, va_idx in skf.split(labeled, bins):\n",
        "        tr = labeled.iloc[tr_idx].copy()\n",
        "        va = labeled.iloc[va_idx].copy()\n",
        "        oof[va_idx] = impute_prices(tr, va, TARGET_COL)\n",
        "\n",
        "    # evaluate CV performance\n",
        "    metrics = eval_metrics(y, oof)\n",
        "    print(f\"CV (Hierarchical Median Imputation) | MAE={metrics['MAE']:.4f} | RMSE={metrics['RMSE']:.4f}\")\n",
        "\n",
        "    # final imputation for missing values using full labeled data\n",
        "    pred_missing = impute_prices(labeled, unlabeled, TARGET_COL)\n",
        "    pred_missing = np.clip(pred_missing, 0, None)\n",
        "\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # overwrite the target column with imputed values where missing\n",
        "    df_out.loc[missing_mask, TARGET_COL] = pred_missing\n",
        "\n",
        "    # track origin of price value (original vs imputed)\n",
        "    df_out[\"Preisquelle\"] = np.where(missing_mask, \"imputiert\", \"original\")\n",
        "\n",
        "    # derive 3-tier price category (tertiles)\n",
        "    labels_de = [\"niedrig\", \"mittel\", \"hoch\"]\n",
        "    df_out[\"Preiskategorie\"] = pd.qcut(df_out[TARGET_COL].astype(float), q=3, labels=labels_de)\n",
        "\n",
        "    # save final dataset\n",
        "    df_out.to_excel(OUT_PATH, index=False)\n",
        "    print(f\"Saved: {OUT_PATH}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
